#!/usr/bin/env python
# coding: utf-8

# # DS/CMPSC 410 Fall 2022
# ## Instructor: Professor John Yen
# ## TA: Rupesh Prajapati and Haiwen Guan
# ## LA: Zining Yin
# ## Lab 4 RDD-based Join, Partitioning, and Performance 
# ## The goals of this lab are for you to be able to
# ## - Use data partitioning to improve scalability 
# ## - Make decisions regarding types of RDD join for integrating data
# ## - Choose transformations and actions that can benefit, or be benefited by, data partitioning
# ## - Apply the obove to compute hastag counts of Tweets after Boston Marathon Bombing (April 19 and April 20).
# ## - Compute changes of the hashtag counts between the two days.
# ##  This lab includes four data sets, as explained below.
# ## Data
# - The first dataset contains Boston Marathon Bombing collected on 4/19/2013.
# - The second dataset contains Boston Marathon Bombing collected on 4/20/2013.
# - To facilitate this analytics task both in the local mode and in the cluster mode, a smaller sampled dataset for each day's tweets has also been provided: sampled_4_19_tweets.csv and sampled_4_20_tweets.csv.  You should use these two data sets for running Spark in the local mode (using Jupyter Notebook).  However, you should change input data for spark-submit (cluster mode) to the big dataset for each day: BMB_4_19_tweets.csv and BMB_4_20_tweets.csv.
# - You should create a Lab4 subdirectory under work, because more space is available under work directory. Download both datasets to your directory `<home>/work/Lab4/`.
# - You should also download the Jupyter Notebook for Lab 4 to the same directory.
# ## Problem
# - The problem we want to solve is to (1) find hashtags that in 4/19/2013 tweets and in 4/20/2013 tweets, and (2) calculate the difference of total hashtag counts of these hashtags (considering both days). You should save these hashtags together with their counts in a text file.
# ## Two Step
# - You will first use Spark local mode (and Jupyter Notebook) to develop the code for a small sample for each day.
# - After you obtain the result for small datasets, you should then convert the code for local mode into code for cluster mode, and submit the code to ICDS cluster and obtain run-time performance.
# - The Jupyter Notebook below provides more detailed instructions.
# 
# ## Lab 4 Part A: Complete 6 Exercises in this Jupyter Notebook.
# 
# ## Lab 4 Part B: Modify Lab 4A for Big Twitter data and run Spark-submit
# 1. You choose the number of pre-partitioning the data. 
# 2. Modify your code in Lab 4 to take advantage of data partitioning. Export it as Lab4C.py
# 3. Modify your code in Lab4C.py to read data files from the two large datasets, and write your output in your work directory under Lab4.
# 3. Submit Lab4C.py to ICDS cluster to obtain its run-time information.
# 
# ## Submit the following items for Lab 4
# - Completed Jupyter Notebook of Lab 4A
# - Lab4C.py (used for spark-submit)
# - Log file for spark-submit
# - An output file generated by running spark-submit (cluster mode) on Lab4C.py.
# 
# ## Total Number of Exercises: 6 + 1 (Part 4B)
# - Exercise 1: 5 points
# - Exercise 2: 10 points
# - Exercise 3: 10 points
# - Exercise 4: 10 points
# - Exercise 5: 10 points
# - Exercise 6: 10 points
# - Part 4B: 30 points
# ## Total Points: 85 points
# 
# # Due: midnight, September 18, 2022

# In[1]:


import pyspark


# In[2]:


from pyspark import SparkContext


# ## Like Lab 2, ww create a Spark Context object.  
# 
# - Note: We use "local" as the master parameter for ``SparkContext`` in this notebook so that we can run and debug it in ICDS Jupyter Server.  However, we need to remove ``"master="local",``later when you convert this notebook into a .py file for running it in the cluster mode.

# In[3]:


sc=SparkContext(appName="Lab4")
#sc


# # Exercise 1 (5 points)  Add your name below 
# ## Answer for Exercise 1
# - Your Name: Kashish Gujral

# ## This lab builds on Lab 3, and includes three steps:
# - Step 1: Compute (and save) hashtag counts for April 19th tweets. (This is similar to Lab 2).
# - Step 2: Compute (and save) hashtag counts for April 20th tweets. (This is also similar to Lab 2).
# - Step 3: Combine the hashtag counts of two days, compute their difference, sort based on the difference. Save the hashtag count difference. 

# # Step 1 Compute (and save) hashtag counts for April 19th tweets.

# # Exercise 2 (10 points) 
# ## Complete the path and run the code below to read the file "sampled_4_19_tweets.csv" from your Lab4 directory.

# In[4]:


tweets_RDD = sc.textFile("/storage/home/kmg6272/work/Lab4/sampled_4_19_tweets.csv")
tweets_RDD


# # Exercise 3 (10 points) 
# ## Like Lab 3, complete and execute the code below, which computes the total count of hashtags in the input tweets, sort them by count (in descending order), and save them in an output directory:
# - (a) Uses flatMap to "flatten" the list of tokens from each tweet (using split function) into a very large list of tokens.
# - (b) Filter the token for hashtags.
# - (c) Count the total number of hashtags in a way similar to Lab 2.
# - (d) Sort the hashtag count in descending order.
# - (e) Save the sorted hashtag counts in an output directory.

# In[5]:


tokens_RDD = tweets_RDD.flatMap(lambda line: line.strip().split(" "))


# In[6]:


hashtag_RDD = tokens_RDD.filter(lambda x: x.startswith("#"))


# In[7]:


hashtag_1_RDD = hashtag_RDD.map(lambda x: (x, 1))


# In[8]:


hashtag_count_RDD = hashtag_1_RDD.reduceByKey(lambda x, y: x+y , 5 )


# In[9]:


sorted_hashtag_count_RDD = hashtag_count_RDD.sortBy(lambda pair: pair[1] , ascending=False)


# ### Note: You need to complete the path with your output directory. 
# ### Note: You also need to change the directory names (e.g., replace "_sampled" with "_cluster") before you convert this notebook into a .py file for submiting it to ICDS cluster.  

# In[10]:


output_path = "/storage/home/kmg6272/work/Lab4/sorted_BMB_hashtag_count_4_19_cluster.txt" 
#sorted_hashtag_count_RDD.saveAsTextFile(output_path)


# # Step 2 Compute and save hashtag counts for April 20th tweets.

# # Exercise 4 (10 points) 
# Complete the path and the code below to read the file "sampled_4_20_tweets.csv" from your Lab4 directory to compute hashtag counts.

# In[11]:


tweets2_RDD = sc.textFile("/storage/home/kmg6272/work/Lab4/sampled_4_20_tweets.csv")
tweets2_RDD


# # Exercise 5 (10 points) 
# ## Like Exercise 3, complete and execute the code below, which computes the total count of hashtags in the input tweets, sort them by count (in descending order), and save them in an output directory:
# - (a) Uses flatMap to "flatten" the list of tokens from each tweet (using split function) into a very large list of tokens.
# - (b) Filter the token for hashtags.
# - (c) Count the total number of hashtags in a way similar to Lab 2.
# - (d) Sort the hashtag count in descending order.
# - (e) Save the sorted hashtag counts in an output directory.

# In[12]:


tokens2_RDD = tweets2_RDD.flatMap(lambda line: line.strip().split(" "))


# In[13]:


hashtag2_RDD = tokens2_RDD.filter(lambda x: x.startswith("#"))


# In[14]:


hashtag2_1_RDD = hashtag2_RDD.map(lambda x: (x,1))


# In[15]:


hashtag2_count_RDD = hashtag2_1_RDD.reduceByKey(lambda x, y: x+y , 5)


# In[16]:


sorted_hashtag2_count_RDD = hashtag2_count_RDD.sortBy(lambda pair: pair[1], ascending=False)


# ### Note: You need to complete the path with your output directory. 
# ### Note: You also need to change the directory names (e.g., replace "_sampled" with "_cluster") before you convert this notebook into a .py file for submiting it to ICDS cluster.  

# In[17]:


output_path2 = "/storage/home/kmg6272/work/Lab4/sorted_BMB_hashtag_count_4_20_cluster.txt" 
sorted_hashtag2_count_RDD.saveAsTextFile(output_path2)


# # Step 3 Combine the hashcount of two days, compute their difference, and save sorted difference of hashtag counts.

# ## Exercise 6 (10 points) Complete and run the code below to (1) join the two hash_count key value pairs RDDs on hashtag (i.e., key), and (2) compute the difference of the hashtag counts between the two days.

# In[18]:


hashtag_count_RDD.take(3)


# In[19]:


hashtag2_count_RDD.take(3)


# In[21]:


joined_hashtag_count_RDD = hashtag_count_RDD.leftOuterJoin(hashtag2_count_RDD)


# In[22]:


joined_hashtag_count_RDD.take(5)


# In[23]:


def tran_none(x):
    if (x==None) :
        return(0)
    else:
        return(x)


# In[24]:


hashtag_count_diff_RDD = joined_hashtag_count_RDD.map(lambda x: (x[0], tran_none(x[1][0])-tran_none(x[1][1])))


# In[25]:


hashtag_count_diff_RDD.take(5)


# In[26]:


sorted_hashtag_count_diff_RDD = hashtag_count_diff_RDD.sortBy(lambda x: x[1], ascending = False)


# In[27]:


sorted_hashtag_count_diff_RDD.take(5)


# In[28]:


output_path3 = "/storage/home/kmg6272/work/Lab4/sorted_BMB_hashtag_diff_sampled.txt" 
sorted_hashtag_count_diff_RDD.saveAsTextFile(output_path3)


# In[29]:


sc.stop()


# In[ ]:




