#!/usr/bin/env python
# coding: utf-8

# # DS/CMPSC 410 Fall 2022
# # Instructor: Professor John Yen
# # TA: Rupesh Prajapati, Haiwen Guan
# # LA: Zining Yin
# 
# # Lab 8 Decision Tree Learning Using ML Pipeline, Visualization, and Hyperparameter Tuning
# 
# ## The goals of this lab are for you to be able to
# - Understand the function of the different steps/stages involved in Spark ML pipeline
# - Be able to construct a decision tree using Spark ML machine learning module
# - Be able to generate a visualization of Decision Trees
# - Be able to perform automated hyper-parameter tuning for Decision Trees 
# - Be able to apply .persist() to suitable DataFrames and evaluate its impact on computation time.
# 
# ## The data set used in this lab is a Breast Cancer diagnosis dataset.
# 
# ## This lab consists of part A (local mode) and part B (cluster mode)
# - Part A: complete the code and Exercises 1, 2, 3, 4 , 5, 6 in this Jupyter Notebook 
# - Part B: Exercise 7 and 8 are for running in the cluster mode without persist (Exercise 7) and with persist (Exercise 8) on two DataFrames. Exercise 9 is for discussing the run-time performance difference with and without the persist (on two DataFrames).
# 
# ## Submit the following items for Lab 8 (DT)
# - Completed Jupyter Notebook of Lab 8 (in HTML format)
# - A .py file (e.g., Lab8DT.py) and logfile for running ONLY part 1 and 6 of this in cluster 
# - A .py file (e.g., Lab8DT_P.py) and logfile for running ONLY part 1 and 6 of this in cluster, with persist() on two DataFrames.
# - Log file for running pbs-spark-submit on Lab8DT.py
# - Log file for running pbs-spark-submit on Lab8DT_P.py
# - The output file that contains the result of hyperparameter tuning.
# - A visualization of the Decision Tree generated by the best hyperparameters in the local mode.
# 
# ## Total Number of Exercises: 100
# - Exercise 1: 5 points
# - Exercise 2: 5 points
# - Exercise 3: 10 points  
# - Exercise 4: 10 points 
# - Exercise 5: 10 points
# - Exercise 6: 10 points
# - Exercise 7: 20 points
# - Exercise 8: 20 points
# - Exercise 9: 10 points
# ## Total Points: 100 points
# 
# # Due: midnight, Oct 23, 2022
# # Bonus for Early Submission before midnight October 21, 2022: 10 points

# # IMPORTANT: DO THE FOLLOWING BEFORE EXECUTING any cell in this Notebook.
# # Load and set up the Python files for this Lab
# 1. Create a `Lab8DT` subdirectory in the `work` directory of your ICDS-ROAR home directory.
# 2. Create a subdirectory under "Lab8DT" called `decision_tree_plot` (named the directory EXACTLY this way).
# 4. Upload the following three files in Module 8 from Canvas to the `decision_tree_plot` directory
# - decision_tree_parser.py
# - decision_tree_plot.py
# - tree_template.jinjia2
# 5. After you have completed the steps above, upload this Notebook to the `Lab8DT` directory.
# 6. Upload the data file `breast-cancer-wisconsin.data.txt` from module 8 in Canvas.
# 
# # AFTER you have completed the steps above, follow the instructions below and execute the PySpark code cell by cell below. Make modifications as required.

# In[1]:


import pyspark
import pandas as pd
import csv


# ## Notice that we use PySpark SQL module to import SparkSession because ML works with SparkSession
# ## Notice also the different methods imported from ML and three submodules of ML: classification, feature, and evaluation.

# In[2]:


from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType
import pyspark.sql.functions as F
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator


# ## The following two lines import relevant functions from the two python files you uploaded into the decision_tree_plot subdirectory.

# In[3]:


from decision_tree_plot.decision_tree_parser import decision_tree_parse
from decision_tree_plot.decision_tree_plot import plot_trees


# ## The first part of this lab runs Spark in the local mode.
# ## The second part of this Lab runs Spark in the cluster mode.
# ## Notice we are creating a SparkSession, not a SparkContext, when we use ML pipeline.
# ## The "getOrCreate()" method means we can re-evaluate this without a need to "stop the current SparkSession" first (unlike SparkContext).

# In[4]:


ss=SparkSession.builder.appName("lab 8 DT").getOrCreate()


# ## Exercise 1: (5 points) Enter your name below:
# - My Name: Kashish Gujral

# ## As we have seen in Lab 4, SparkSession offers a way to read a CSV/text file with the capability to interpret the first row as being the header and infer the type of different columns based on their values.

# ## Exercise 2: (5 points) Complete the following path with the path for your home directory.  

# In[5]:


bc_schema = StructType([ StructField("id", IntegerType(), False ),                         StructField("clump_thickness", IntegerType(), False),                         StructField("unif_cell_size", IntegerType(), False ),                         StructField("unif_cell_shape", IntegerType(), False ),                         StructField("marg_adhesion", IntegerType(), False),                         StructField("single_epith_cell_size", IntegerType(), False),                         StructField("bare_nuclei", IntegerType(), False),                        StructField("bland_chrom", IntegerType(), False),                         StructField("norm_nucleoli", IntegerType(), False),                         StructField("mitoses", IntegerType(), False),                         StructField("class", StringType(), False)                            ])


# In[6]:


data = ss.read.csv("/storage/home/kmg6272/work/Lab8DT/breast-cancer-wisconsin.data.txt", schema=bc_schema, header=True, inferSchema=False)


# # Part 1 Data Cleaning Using DataFrame

# In[7]:


#data.printSchema()


# In[8]:


#data.show(5)


# In[9]:


from pyspark.sql.functions import col
class_count = data.groupBy(col("class")).count()
#class_count.show()


# # Detecting and Filtering Rows with missing values

# In[10]:


#data.filter(col("bare_nuclei").isNull()).show()


# In[11]:


data2 = data.filter(col("bare_nuclei").isNotNull())


# In[12]:


from pyspark.sql.functions import col
class_count2 = data2.groupBy(col("class")).count()
#class_count2.show()


# # Note: We will use `data2` (rather than `data`) in all of the remaining lab, because `data2` does not contain missing/null values.




# # Part 6 Automated Hyperparameter Tuning for Decision Tree

# ## Exercise 5: (10 points)  
# - Complete the code below to perform hyper parameter tuning of Decision Tree (for two parameters: max_depth and minInstancesPerNode)

# In[46]:


input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion',                   'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']


# In[48]:


trainingData, testingData= data2.randomSplit([0.75, 0.25], seed=1234)
model_path="/storage/home/kmg6272/work/Lab8DT/DTmodel_vis"


# In[50]:


## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings
hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )
# initialize index to the hyperparam_eval_df to 0
index =0 
# initialize lowest_error
highest_testing_f1 = 0
# Set up the possible hyperparameter values to be evaluated
max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
minInstancesPerNode_list = [2, 3, 4, 5, 6]
labelIndexer = StringIndexer(inputCol="class", outputCol="indexedLabel").fit(data2)
assembler = VectorAssembler( inputCols=input_features, outputCol="features")
labelConverter = IndexToString(inputCol = "prediction", outputCol="predictedClass", labels=labelIndexer.labels)
for max_depth in max_depth_list:
    for minInsPN in minInstancesPerNode_list:
        seed = 37
        # Construct a DT model using a set of hyper-parameter values and training data
        dt= DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="features", maxDepth= max_depth, minInstancesPerNode= minInsPN)
        pipeline = Pipeline(stages=[labelIndexer, assembler, dt, labelConverter])
        model = pipeline.fit(trainingData)
        training_predictions = model.transform(trainingData)
        testing_predictions = model.transform(testingData)
        evaluator = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="f1")
        training_f1 = evaluator.evaluate(training_predictions)
        testing_f1 = evaluator.evaluate(testing_predictions)
        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.
        # The best model will have a value 1000
        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  
        index = index +1
        if testing_f1 > highest_testing_f1 :
            best_max_depth = max_depth
            best_minInsPN = minInsPN
            best_index = index -1
            best_parameters_training_f1 = training_f1
            best_DTmodel= model.stages[2]
            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)
            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           
            highest_testing_f1 = testing_f1
print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ',       best_minInsPN, ', testing f1 = ', highest_testing_f1) 
column = dict([(str(idx), i) for idx, i in enumerate(input_features)])


# In[51]:


plot_trees(best_tree, column = column, output_path = '/storage/home/kmg6272/work/Lab8DT/bestDTtree_10_19_2022.html')


# In[52]:


# Store the Testing RMS in the DataFrame
hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]


# ## Exercise 6 (10 points)
# ### Complete the path below to save the result of your hyperparameter tuning in a directory.
# 
# ## Notice: Modify the output path before you export this to a .py file for running in cluster mode.  
# ## Notice: Remember to change the output_path directory after each spark-submit in cluster. Otherwise, the spark_submit will re-write (destroy) your previous hyper-parameter tuning result.

# In[53]:


output_path = "/storage/home/kmg6272/work/Lab8DT/Lab8DT_HyperparamsTuning_10-19-2022.csv"
hyperparams_eval_df.to_csv(output_path)  


# In[ ]:


ss.stop()


# # Exercise 7 (20 points) 
# ## Modify this Notebook to comment out (or remove) Part 2, 3, and 4, 5 and modify it for running spark-submit in cluster mode.  Export it as a .py file (e.g., Lab8DT.py). Record the computation time below:

# ## Answer to Exercise 7:

# # Exercise 8 (20 points)
# ## Modify .py file used in Exercise 7 to add `persist()` to two DataFrame for enhanced scalability of the code. (a) Record the computation time below. (b) Compare the computation time with and without persist.

# ## Answer to Exercise 8: 

# # Exercise 9 (10 points)
# ## Discuss the difference of the run-time of Exercise 7 and 8, based on the run-time information in their log files. 

# ## Answer to Exercise 9:

# In[ ]:




